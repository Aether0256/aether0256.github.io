<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Iridium-文献阅读</title>
    <url>/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<p>论文标题：<a href="https://dl.acm.org/doi/abs/10.1145/2829988.2787505">Low Latency Geo-distributed Data Analytics</a> | <strong>低时延地域分布式数据分析</strong></p>
<p>会议&#x2F;期刊：SIGCOMM 2015</p>
<p>第一作者：Qifan Pu [MSR&#x2F;UCB]</p>
<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>为了能够给用户提供高质量的互联网服务，云计算厂商在世界各地广泛部署数据中心和服务器集群，这些站点每天都在生产着大量数据。运营商需要通过分析这些数据以实现实时的广告推送、攻击检测、集群健康状况维护等工作，因此，<strong>能够对这些数据进行低延迟分析越来越重要</strong>。</p>
<p>对分布在不同地理位置的数据进行分析，有两种主流方案。一种是将所有数据上传至一个数据中心后再进行分析，另一种将用于数据中心内部的分析框架直接应用于广域网中。前者将数据集中的过程需要花费较多的时间，而且由于法规限制，难以随意进行数据跨区域传输，一些系统日志详情等价值较低的数据通过广域网传输也会带来不必要的开销。后者未能考虑广域网低而多变的带宽，直接套用也会带来较大的时延。<strong>两种既有主流方案不能达到低延迟的要求。</strong></p>
<h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p>为实现<strong>对分布在不同地理位置的数据集进行低延迟的数据分析</strong>，本文提出了Iridium框架，其核心理念是对查询请求重新放置数据集以及重新分配计算任务。以map-reduce计算任务为例，系统工作流程如图所示。</p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/1.png" alt="image-20231027200351534" style="zoom:50%;">

<p>管理员向系统提交查询请求，Global Manager将用户的查询脚本转化为一个包含多个步骤的有向无环图(DAG)，每一步都包含多个并行的计算任务，交由Site Manager执行计算，同时负责协调不同Site之间的任务分配，监控数据分布情况，并维持数据的可用性和一致性。Site Manager监控本地相关资源状况并向Global Manager周期性上传，</p>
<p>相对于接入带宽，CPU和IO资源更为丰富，加之数据的局部性和Spark使用内存进行读写，map操作阶段时间较短，需要进行all-to-all shuffle操作的reduce阶段则需要在各个数据中心之间传输数据，<strong>是决定查询响应时间长短的主要因素</strong>。因此，系统在分配计算任务时，考虑了各站点的数据量以及上下行链路带宽状况，避免让低带宽链路传输大量数据的情况发生。</p>
<p>此外，在数据仅保留在站点本地的情况下，任务分配的最好结果由数据分布情况决定，而数据查询请求一般是周期性发生的，具备可预测性。因此，在数据集产生后到下一次请求到来之前的时间，通过将瓶颈站点上的数据拷贝到其它站点再进行任务分配，能够达到进一步减小时延的效果。</p>
<p>把任务分配和数据拷贝两个步骤进行联合建模优化难以求得最优解，因此，系统将二者分开，在请求到来前，设计启发式方法拷贝部分数据集，在请求执行过程中，通过求解线性规划模型合理分配计算任务。此外，为避免拷贝行为带来过多的带宽开销，系统还设计了调节器用于控制预算。</p>
<h3 id="任务分配算法"><a href="#任务分配算法" class="headerlink" title="任务分配算法"></a>任务分配算法</h3><p>设计任务分配算法需要基于以下几个前提假设。</p>
<ol>
<li>数据中心网络瓶颈仅存在于接入网，核心网络带宽充足，时延极低；</li>
<li>数据中心计算和存储资源充足，计算时间极短。</li>
<li>由于链路性能不同及背景流量的存在，各数据中心上下行链路带宽差异性较大。</li>
<li>数据可以在任意站点产生，一个数据集会分布在多个站点上，且数据集中数据特征分布相同。</li>
<li>计算任务可任意微分。</li>
</ol>
<p>进行任务分配的一个案例如下所示。</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Site-1</th>
<th align="center">Site-2</th>
<th align="center">Site-3</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Input Data(MB),I</td>
<td align="center">300</td>
<td align="center">240</td>
<td align="center">240</td>
</tr>
<tr>
<td align="center">Intermediate Data(MB),S</td>
<td align="center">150</td>
<td align="center">120</td>
<td align="center">120</td>
</tr>
<tr>
<td align="center">Uplink(MB&#x2F;s),U</td>
<td align="center">10</td>
<td align="center">10</td>
<td align="center">10</td>
</tr>
<tr>
<td align="center">Downlink(MB&#x2F;s),D</td>
<td align="center">1</td>
<td align="center">10</td>
<td align="center">10</td>
</tr>
</tbody></table>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231027204923809.png" alt="image-20231027204923809" style="zoom:50%;">

<p>在包含三个数据中心的场景中，原始数据即Input Data，经过map操作之后得到Intermediate Data，需要根据任务分配比例在各数据中心间完成传输。按照Spark的默认规则，三个站点各自完成1&#x2F;3的计算任务，这就会造成每个站点上传2&#x2F;3的数据，同时下载来自另外两个节点各自1&#x2F;3的数据。由于Site-1的下行链路带宽较低，作为瓶颈，共需要80s才能完成所有的数据下载。</p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231027205312400.png" alt="image-20231027205312400" style="zoom:50%;">

<p>而如果调整一下任务分配比例，修改为三个站点各自执行占总体0.05&#x2F;0.475&#x2F;0.475的任务，就避免了Site-1的下行链路下载过多的数据，从而将数据传输时间缩短至14.25s。而相比于带宽开销最小的情况，即将Site-2和Site-3上的数据全部传输给Site-1，开销240MB，此方案仅提高了12%的带宽使用(268.5MB)，而缩短了17倍的响应时间(240s-&gt;14.25s)。</p>
<p>基于此案例及其背后的思想，可以建立线性规划模型如下。</p>
<table>
<thead>
<tr>
<th align="center">符号</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">$I_i$</td>
<td align="center">站点$i$输入数据量</td>
</tr>
<tr>
<td align="center">$S_i$</td>
<td align="center">站点$i$map操作后中间数据量</td>
</tr>
<tr>
<td align="center">$\alpha$</td>
<td align="center">输入数据选择率，$S_i&#x3D;\alpha \cdot I_i$</td>
</tr>
<tr>
<td align="center">$D_i$</td>
<td align="center">站点$i$下行带宽</td>
</tr>
<tr>
<td align="center">$U_i$</td>
<td align="center">站点$i$上行带宽</td>
</tr>
<tr>
<td align="center">$r_i$</td>
<td align="center">站点$i$ reduce任务分配比例</td>
</tr>
<tr>
<td align="center">$T_i^U,T_i^D$</td>
<td align="center">站点$i$中间数据上传&#x2F;下载完成时间</td>
</tr>
</tbody></table>
<p>$$<br>\begin{align}<br>min \ &amp;  z \<br>s.t.   \ &amp;  \forall i:r_i \geq 0 \<br>&amp;  \sum_{i}r_i&#x3D;1 \<br>&amp; \forall i:T_i^U(r_i) \leq z, \ T_i^D(r_i) \leq z \<br>\end{align}<br>$$</p>
<p>对于Spark中的load&#x2F;filter这种任务，仅使用本地数据并且输出数据也在本地；对于join任务，分为需要进行all-to-all shuffle的hash join以及broadcast join两种，前者可直接使用上述reduce任务方案，后者仅需要传输大小固定的小数据表，因此任务分配不会对时延造成影响。</p>
<p>对于一系列DAG任务，由于第一步计算之后产生的中间数据是主要部分，之后步骤的中间数据量会迅速下降，因此仅需要按照拓扑排序顺序，使用线性规划逐步进行计算即可。</p>
<h3 id="数据拷贝方案"><a href="#数据拷贝方案" class="headerlink" title="数据拷贝方案"></a>数据拷贝方案</h3><p>同样通过一个案例说明数据拷贝的作用。</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Site-1</th>
<th align="center">Site-2</th>
<th align="center">Site-3</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Input Data(MB),I</td>
<td align="center">240</td>
<td align="center">120</td>
<td align="center">60</td>
</tr>
<tr>
<td align="center">Intermediate Data(MB),S</td>
<td align="center">240</td>
<td align="center">120</td>
<td align="center">60</td>
</tr>
<tr>
<td align="center">Uplink(MB&#x2F;s),U</td>
<td align="center">10</td>
<td align="center">10</td>
<td align="center">10</td>
</tr>
<tr>
<td align="center">Downlink(MB&#x2F;s),D</td>
<td align="center">1</td>
<td align="center">10</td>
<td align="center">10</td>
</tr>
</tbody></table>
<p>假设三个数据中心的数据分布情况如表格所示($\alpha&#x3D;1$)，下一次查询请求将在24s后到来。如果在这24s内什么都不做，即保持数据的分布情况，那么借助任务分配算法中的模型，可以得到最优分配任务之下的最大传输时间为Site-1的数据上传数据，共21.6s。而如果利用这24s，将Site-1的数据全部传输至Site-2，那么任务重新分配之后，中间数据传输时间仅需要5.4s。</p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231027211133541.png" alt="image-20231027211133541" style="zoom:50%;">

<p>由于将数据拷贝和任务分配联合建模难以求得最优解，因此通过启发式算法解决数据拷贝问题。即判断瓶颈节点，并利用请求到来前的时间片段，将数据拷贝至其它节点，算法的核心是拷贝到哪一个站点，以及拷贝多少数据，以及在一个站点存在多个数据集的情况下，选择哪一个数据集进行拷贝。</p>
<p>使用上述数据分布情况，在请求到来之前，分别将数据全部从Site-1移至Site-2或Site-3，并使用任务分配算法，根据结果可知，与全部移动至Site-3的耗时9s相比，全部移动至Site-2仅耗时5.4。</p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231028115039599.png" alt="image-20231028115039599" style="zoom:50%;">

<p>Iridium设计了以下算法，来进行数据集的选择与移动。算法的核心思想是计算每一个数据集的瓶颈站点及在下一个请求到来之前移动此数据集瓶颈站点的价值(value)，价值通过获得的时延减小量与该数据集下一个请求到来时间之比得到，再通过与达到此价值需要移动的数据量之比计算得分，并按照降序排列，从高到低依次进行移动。</p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231028120536415.png" alt="image-20231028120536415" style="zoom:50%;">

<p><strong>查询预测</strong>：Iridium记录目前所有已经到达的请求及其时间，预测自最后一次查询到来之后，后续请求还会以之前的时间间隔特征到来。这一预测方案会在起初低估请求数量，之后逐渐增加，并且往往会在到达过预测的临界值之前，使数据分布达到稳定。</p>
<p><strong>数据拷贝和任务流量冲突处理</strong>：Iridium假设各流量之间公平享有带宽，当二者冲突时，如果数据拷贝带来的时间收益大于对当前任务造成的影响，则允许数据拷贝进行，否则跳过当前数据集，进行下一个数据集的拷贝。</p>
<h3 id="带宽开销控制"><a href="#带宽开销控制" class="headerlink" title="带宽开销控制"></a>带宽开销控制</h3><p>Iridium假设带宽的基准开销为W，强化因子为B。每一个数据集允许使用的带宽M&#x3D;0，有新的数据集产生时，M+&#x3D;W*B。当数据发生了拷贝之后，M会减去移动的数据量，当其为0时不再移动。测试表明，当B为1时，Iridium相比最佳的带宽优化方案，能够将请求提速两倍多；当B为1.3时，能够达到无带宽约束下时间收益的90%</p>
<h2 id="系统实现"><a href="#系统实现" class="headerlink" title="系统实现"></a>系统实现</h2><p>Iridium系统基于Spark实现，重写了它的调度器，并且内置了Gurobi求解器模块。对任务的分配模型修改为整数线性规划，并且每个Job仅调用一次该计算；在拷贝数据时，计算最优任务分配仍使用前文描述的线性规划模型。</p>
<p>系统沿用了Spark的高性能分布式文件系统(HDFS)。请求通过Spark Manager统一接口提交，支持SQL和Streaming两种Spark扩展。</p>
<p>系统测试表明，可用带宽在分钟级别上是稳定的，因此Iridium每分钟会进行带宽的测量。并且通过额外的测量监控数据流和任务流的带宽占用情况，以及检测背景流量。</p>
<p>Iridium通过经验预估每一种请求的$\alpha$值，准确率可达92%。</p>
<h2 id="系统测试"><a href="#系统测试" class="headerlink" title="系统测试"></a>系统测试</h2><p>测试环境：8个位于不同地理位置的EC2服务器，以及使用一个区域内的EC2模拟30个节点。</p>
<p>测试数据：Conviva Video Analytics；Microsoft Bing Edge Dashboard；TPC-DS Bechemark；AMPLab Big-Data</p>
<p>Baseline：In-place方案和聚合方案</p>
<p><strong>时延节省收益</strong></p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231028142441075.png" alt="image-20231028142441075" style="zoom:50%;">

<p><strong>增加请求预测和冲突避免后的时延收益</strong></p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231028142530502.png" alt="image-20231028142530502" style="zoom:50%;">

<p><strong>两种技术独立使用与结合使用获得的时延收益比较</strong></p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231028142832075.png" alt="image-20231028142832075" style="zoom:50%;">

<p><strong>数据集不同分布情况对结果的影响</strong></p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231028143103854.png" alt="image-20231028143103854" style="zoom:50%;">

<p><strong>不同间隔时间估计方法对结果的影响</strong></p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231028143148322.png" alt="image-20231028143148322" style="zoom:50%;">

<p><strong>不同带宽开销对时延收益的影响</strong></p>
<img src="/2023/10/23/Iridium-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20231028143305960.png" alt="image-20231028143305960" style="zoom:50%;">

<h2 id="讨论总结"><a href="#讨论总结" class="headerlink" title="讨论总结"></a>讨论总结</h2><p>系统存在多个需要改进之处，主要为假设中的<strong>不考虑计算与存储资源约束</strong>，<strong>不考虑WAN拓扑</strong>，以及使用的<strong>局部最优的贪心算法</strong>。相关工作包括分布式数据处理、周期性数据分析任务响应时延、广域网调度三个方向。</p>
]]></content>
      <categories>
        <category>文献阅读</category>
      </categories>
      <tags>
        <tag>大数据处理</tag>
        <tag>广域网</tag>
      </tags>
  </entry>
</search>
